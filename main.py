import os
import ollama
import json
import yaml
import re  # Â¡Necesario para las mejoras de limpieza!

# --- ConfiguraciÃ³n ---
SPEC_PATH = 'spec/openapi.yaml'
PROMPT_PATH = 'prompt/prompt2mejorado.txt'
REQUIREMENTS_PATH = 'requirements.txt'  # <--- NUEVA RUTA DE ARCHIVO
OUTPUT_DIR = 'output'
OUTPUT_FILE_NAME = 'ai_generated_tests2mejorado.py'
OUTPUT_PATH = os.path.join(OUTPUT_DIR, OUTPUT_FILE_NAME)

# AsegÃºrate de que este modelo existe y estÃ¡ descargado con `ollama run llama3:8b`
MODEL = 'llama3.2:1b'


# --- Funciones de Utilidad ---
def load_file_content(file_path):
    """Carga el contenido de un archivo."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        print(f"âŒ Error: Archivo no encontrado en {file_path}")
        exit(1)
    except Exception as e:
        print(f"âŒ Error al leer {file_path}: {e}")
        exit(1)


def save_output(text):
    """Guarda el texto generado en un archivo."""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
        f.write(text)
    print(f"âœ” CÃ³digo generado guardado en {OUTPUT_PATH}")


def extract_python_code(raw_output):
    """
    Extrae bloques de cÃ³digo Python de la salida de la IA,
    limpia comentarios no deseados y reordena importaciones.
    """

    # --- Paso 1: Extraer el bloque de cÃ³digo principal (si hay Markdown) ---
    # Esto maneja si la IA aÃºn incluye bloques ```python```
    match = re.search(r'```python\n(.*?)```', raw_output, re.DOTALL)
    if match:
        code_content = match.group(1).strip()
    else:
        # Si no hay bloques Markdown, asumimos que toda la salida es el cÃ³digo
        code_content = raw_output.strip()

    # --- Paso 2: Limpieza de lÃ­neas introductorias/redundantes y comentarios no deseados ---
    lines = code_content.split('\n')
    cleaned_lines_temp = []

    # Patrones para identificar y eliminar ruido
    noise_patterns = [
        r"^\s*#\s*Replace with.*",
        r"^\s*#\s*TODO:.*",
        r"^\s*#\s*Filename:.*",
        r"^\s*#\s*File name:.*",
        r"^\s*#\s*test_client\.py.*",  # Ejemplo especÃ­fico que mencionaste
        r"^\s*#\s*This script generates.*",
        r"^\s*#\s*Below is the generated.*",
        r"^\s*#\s*Generated by an AI.*",
        r"^\s*```python.*",  # Asegurarse de eliminar cualquier ```python remanente
        r"^\s*```.*",  # Asegurarse de eliminar cualquier ``` remanente
        r"^\s*here is the python code.*",
        r"^\s*as a qa automation expert.*",
        r"^\s*i've generated the following tests:.*"
    ]

    # Compilar los patrones para mayor eficiencia
    compiled_noise_patterns = [re.compile(p, re.IGNORECASE) for p in noise_patterns]

    for line in lines:
        is_noise = False
        for pattern in compiled_noise_patterns:
            if pattern.match(line):
                is_noise = True
                break

        if not is_noise:
            cleaned_lines_temp.append(line)

    code_content = "\n".join(cleaned_lines_temp).strip()

    # --- Paso 3: Extraer y reordenar importaciones ---
    imports = []
    other_code_lines = []

    # ExpresiÃ³n regular para encontrar lÃ­neas de importaciÃ³n de Python
    import_pattern = re.compile(
        r'^(import \w+(\.\w+)*(\s*as\s*\w+)?|from \w+(\.\w+)* import (\w+(\s*as\s*\w+)?(?:,\s*\w+(\s*as\s*\w+)?)*|\*))'
    )

    for line in code_content.split('\n'):
        if import_pattern.match(line):
            imports.append(line)
        else:
            other_code_lines.append(line)

    # Eliminar duplicados de importaciones y mantener el orden de primera apariciÃ³n
    # Esto asegura que si hay 'import os' y luego 'import os', solo aparece una vez.
    unique_imports = []
    seen_imports = set()
    for imp_line in imports:
        if imp_line not in seen_imports:
            unique_imports.append(imp_line)
            seen_imports.add(imp_line)

    # Opcional: Ordenar las importaciones alfabÃ©ticamente dentro de sÃ­ mismas
    # Si quieres agrupar por tipo (estÃ¡ndar, terceros, locales), necesitarÃ­as lÃ³gica mÃ¡s avanzada
    unique_imports.sort()

    # --- Paso 4: Reconstruir el cÃ³digo ---
    final_code_parts = []
    if unique_imports:
        final_code_parts.extend(unique_imports)
        final_code_parts.append("")  # AÃ±adir una lÃ­nea en blanco despuÃ©s de las importaciones para PEP 8

    # AÃ±adir el resto del cÃ³digo
    final_code_parts.extend(other_code_lines)

    # Unir todas las partes y limpiar posibles lÃ­neas en blanco extra al inicio/final
    return "\n".join(final_code_parts).strip()


def run_model(prompt_content):
    """Ejecuta el modelo de IA con el prompt dado y limpia la salida."""
    print("ðŸš€ Ejecutando IA...")
    try:
        response_generator = ollama.chat(model=MODEL, messages=[{'role': 'user', 'content': prompt_content}],
                                         stream=True)

        full_response_content = ""
        for chunk in response_generator:
            if 'content' in chunk['message']:
                full_response_content += chunk['message']['content']
                # Opcional: imprimir la respuesta en tiempo real para depuraciÃ³n
                # print(chunk['message']['content'], end='', flush=True)
        print("\nâœ… IA finalizada.")

        # --- Limpieza de la salida de la IA ---
        cleaned_code = extract_python_code(full_response_content)
        return cleaned_code

    except ollama._types.ResponseError as e:
        print(f"âŒ Error de Ollama: {e}")
        print(f"AsegÃºrate de que el modelo '{MODEL}' estÃ© descargado y el servidor Ollama estÃ© en ejecuciÃ³n.")
        exit(1)
    except Exception as e:
        print(f"âŒ Error inesperado al ejecutar la IA: {e}")
        exit(1)


# --- FunciÃ³n Principal ---
def main():
    """FunciÃ³n principal para orquestar la generaciÃ³n de cÃ³digo."""
    # 1. Cargar el contenido de la especificaciÃ³n OpenAPI
    openapi_spec_content = load_file_content(SPEC_PATH)
    print(f"âœ” Cargado {SPEC_PATH}")

    # 2. Cargar el contenido de requirements.txt
    requirements_content = load_file_content(REQUIREMENTS_PATH) # <--- NUEVA CARGA
    print(f"âœ” Cargado {REQUIREMENTS_PATH}") # <--- NUEVO MENSAJE

    # 3. Cargar el prompt base
    base_prompt_content = load_file_content(PROMPT_PATH)
    print(f"âœ” Cargado {PROMPT_PATH}")

    # 4. Combinar el prompt con las especificaciones
    full_prompt = base_prompt_content.replace('{{SPEC_CONTENT}}', openapi_spec_content)
    full_prompt = full_prompt.replace('{{REQS}}', requirements_content) # <--- NUEVO REEMPLAZO

    # 5. Ejecutar el modelo de IA
    generated_code = run_model(full_prompt)

    # 6. Guardar el cÃ³digo generado
    save_output(generated_code)


if __name__ == "__main__":
    main()